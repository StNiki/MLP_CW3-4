{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coursework 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each block was called from a run.sh script individually after adjusting its parameters\n",
    "# The results were saved in txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Baseline training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pickle as pcl\n",
    "from mlp.data_providers import CIFAR10DataProvider, CIFAR100DataProvider\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "#matplotlib inline\n",
    "#from IPython.display import Image\n",
    "\n",
    "\n",
    "assert 'MLP_DATA_DIR' in os.environ, (\n",
    "    'An environment variable MLP_DATA_DIR must be set to the path containing'\n",
    "    ' MLP data before running script.')\n",
    "assert 'OUTPUT_DIR' in os.environ, (\n",
    "    'An environment variable OUTPUT_DIR must be set to the path to write'\n",
    "    ' output to before running script.')\n",
    "\n",
    "# Run a standard CNN to get a model that overfits\n",
    "\n",
    "models = {} # store everything\n",
    "\n",
    "# Helping functions\n",
    "\n",
    "def set_wb(wb,mp):\n",
    "    p = 32/(mp*2)\n",
    "    weights = {\n",
    "        'wc1': weight_variable([5, 5, 3, wb]),\n",
    "        'wc2': weight_variable([5, 5, wb, wb*2]),\n",
    "        'wc3': weight_variable([5, 5, wb*2, wb*2]),\n",
    "        'wd1': weight_variable([p*p*(wb*2), wb**3]),\n",
    "        'out': weight_variable([wb**3, 10])\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'bc1': bias_variable([wb]),\n",
    "        'bc2': bias_variable([wb*2]),\n",
    "        'bc3': bias_variable([wb*2]),\n",
    "        'bd1': bias_variable([wb**3]),\n",
    "        'out': bias_variable([10])\n",
    "    }\n",
    "    return weights, biases\n",
    "\n",
    "def write_to_file(newfile, run_info):\n",
    "    with open(newfile,'w') as outfile:\n",
    "        pcl.dump(run_info,outfile)\n",
    "\n",
    "def read_from_file(newfile):\n",
    "    run_info = pcl.load(open(newfile,'r'))\n",
    "    return run_info\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "# model: CNN/RELU>CNN/RELU>MP>CNN/RELU>MP>FC\n",
    "\n",
    "def conv_net(x, weights, biases):\n",
    "    x = tf.reshape(x, shape=[-1, 32, 32, 3])\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    #conv12 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    #conv13 = conv2d(conv12, weights['wc3'], biases['bc3'])\n",
    "    #conv14 = conv2d(conv13, weights['wc3'], biases['bc3'])\n",
    "\n",
    "    # Max Pooling\n",
    "    #conv1 = maxpool2d(conv1, k=2)\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    # Convolution Layer\n",
    "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n",
    "    # Max Pooling\n",
    "    conv3 = maxpool2d(conv3, k=2)\n",
    "    # Fully connected layer\n",
    "    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "\n",
    "## session\n",
    "\n",
    "num_epoch = 30 \n",
    "fname = \"baseK5-\"\n",
    "\n",
    "train_data = CIFAR10DataProvider('train', batch_size=50)\n",
    "valid_data = CIFAR10DataProvider('valid', batch_size=50)\n",
    "valid_inputs = valid_data.inputs\n",
    "valid_targets = valid_data.to_one_of_k(valid_data.targets)\n",
    "\n",
    "ks = [32]\n",
    "for k in ks:\n",
    "    tf.reset_default_graph()\n",
    "    weightsk, biasesk = set_wb(k,2)\n",
    "    \n",
    "    # define model graph\n",
    "    with tf.name_scope('data'):\n",
    "        inputs = tf.placeholder(tf.float32, [None, 3072], name='inputs')\n",
    "        targets = tf.placeholder(tf.float32, [None, 10], name='targets')\n",
    "    #with tf.name_scope('parameters'):\n",
    "    #    weights = tf.Variable(tf.zeros([3072, 10]), name='weights')\n",
    "    #    biases = tf.Variable(tf.zeros([10]), name='biases')\n",
    "    with tf.name_scope('model'):\n",
    "        outputs = conv_net(inputs, weightsk, biasesk)\n",
    "    with tf.name_scope('error'):\n",
    "        error = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(outputs,targets))\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer().minimize(error)\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(\n",
    "            tf.argmax(outputs, 1), tf.argmax(targets, 1)), tf.float32))\n",
    "\n",
    "    train_accuracy = np.zeros(num_epoch)\n",
    "    train_error = np.zeros(num_epoch)\n",
    "    valid_accuracy = np.zeros(num_epoch)\n",
    "    valid_error = np.zeros(num_epoch)\n",
    "\n",
    "    name = str(k)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    step = 0\n",
    "    for e in range(num_epoch):\n",
    "        for b, (input_batch, target_batch) in enumerate(train_data):\n",
    "\n",
    "            _, batch_error, batch_acc = sess.run(\n",
    "                [train_step, error, accuracy],\n",
    "                feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "            train_error[e] += batch_error\n",
    "            train_accuracy[e] += batch_acc\n",
    "            step += 1\n",
    "        \n",
    "        train_error[e] /= train_data.num_batches\n",
    "        train_accuracy[e] /= train_data.num_batches\n",
    "\n",
    "        valid_error[e], valid_accuracy[e] = sess.run(\n",
    "            [error, accuracy],\n",
    "            feed_dict={inputs: valid_inputs, targets: valid_targets})\n",
    "\n",
    "    stats = (train_error, train_accuracy, valid_error, valid_accuracy)\n",
    "    \n",
    "    ffname = fname+name+\"_10.txt\"\n",
    "    write_to_file(ffname,stats)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropout on the fully connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pickle as pcl\n",
    "from mlp.data_providers import CIFAR10DataProvider, CIFAR100DataProvider\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "#matplotlib inline\n",
    "#from IPython.display import Image\n",
    "\n",
    "\n",
    "assert 'MLP_DATA_DIR' in os.environ, (\n",
    "    'An environment variable MLP_DATA_DIR must be set to the path containing'\n",
    "    ' MLP data before running script.')\n",
    "\n",
    "assert 'OUTPUT_DIR' in os.environ, (\n",
    "    'An environment variable OUTPUT_DIR must be set to the path to write'\n",
    "    ' output to before running script.')\n",
    "\n",
    "# Run a standard CNN to get a model that overfits\n",
    "\n",
    "# Helping functions\n",
    "\n",
    "def set_wb(wb,mp):\n",
    "    weights = {\n",
    "        'wc1': weight_variable([2, 2, 3, 8]),\n",
    "        'wc2': weight_variable([2, 2, 8, 16]),\n",
    "        'wc3': weight_variable([2, 2, 16, 16]),\n",
    "        'wd1': weight_variable([8*8*16, 8**3]),\n",
    "        'out': weight_variable([8**3, 10])\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'bc1': bias_variable([8]),\n",
    "        'bc2': bias_variable([16]),\n",
    "        'bc3': bias_variable([16]),\n",
    "        'bd1': bias_variable([8**3]),\n",
    "        'out': bias_variable([10])\n",
    "    }\n",
    "    return weights, biases\n",
    "\n",
    "def write_to_file(newfile, run_info):\n",
    "    with open(newfile,'w') as outfile:\n",
    "        pcl.dump(run_info,outfile)\n",
    "\n",
    "def read_from_file(newfile):\n",
    "    run_info = pcl.load(open(newfile,'r'))\n",
    "    return run_info\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "# model: CNN/RELU>CNN/RELU>MP>CNN/RELU>MP>FC\n",
    "# DROP ONCE\n",
    "def conv_net(x, weights, biases, drop):\n",
    "    x = tf.reshape(x, shape=[-1, 32, 32, 3])\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv12 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv13 = conv2d(conv12, weights['wc3'], biases['bc3'])\n",
    "    conv14 = conv2d(conv13, weights['wc3'], biases['bc3'])\n",
    "    # Max Pooling\n",
    "    #conv1 = maxpool2d(conv1, k=2)\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv14, weights['wc3'], biases['bc3'])\n",
    "    # Max Pooling\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    # Convolution Layer\n",
    "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n",
    "    # Max Pooling\n",
    "    conv3 = maxpool2d(conv3, k=2)\n",
    "    # Fully connected layer\n",
    "    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, drop)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "\n",
    "## session\n",
    "\n",
    "num_epoch = 30 \n",
    "fname = \"base6CNN-drop-fc\"\n",
    "\n",
    "train_data = CIFAR10DataProvider('train', batch_size=50)\n",
    "valid_data = CIFAR10DataProvider('valid', batch_size=50)\n",
    "valid_inputs = valid_data.inputs\n",
    "valid_targets = valid_data.to_one_of_k(valid_data.targets)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "pb = [0.1,0.25,0.5,0.75,0.9]\n",
    "for p in pb:\n",
    "    tf.reset_default_graph()\n",
    "    weightsk, biasesk = set_wb(8,2)\n",
    "    \n",
    "    # define model graph\n",
    "    with tf.name_scope('data'):\n",
    "        inputs = tf.placeholder(tf.float32, [None, 3072], name='inputs')\n",
    "        targets = tf.placeholder(tf.float32, [None, 10], name='targets')\n",
    "    #with tf.name_scope('parameters'):\n",
    "    #    weights = tf.Variable(tf.zeros([3072, 10]), name='weights')\n",
    "    #    biases = tf.Variable(tf.zeros([10]), name='biases')\n",
    "    with tf.name_scope('model'):\n",
    "        outputs = conv_net(inputs, weightsk, biasesk,p)\n",
    "    with tf.name_scope('error'):\n",
    "        error = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(outputs,targets))\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer().minimize(error)\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(\n",
    "            tf.argmax(outputs, 1), tf.argmax(targets, 1)), tf.float32))\n",
    "\n",
    "    train_accuracy = np.zeros(num_epoch)\n",
    "    train_error = np.zeros(num_epoch)\n",
    "    valid_accuracy = np.zeros(num_epoch)\n",
    "    valid_error = np.zeros(num_epoch)\n",
    "\n",
    "    name = str(p)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    step = 0\n",
    "    for e in range(num_epoch):\n",
    "        for b, (input_batch, target_batch) in enumerate(train_data):\n",
    "\n",
    "            _, batch_error, batch_acc = sess.run(\n",
    "                [train_step, error, accuracy],\n",
    "                feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "            train_error[e] += batch_error\n",
    "            train_accuracy[e] += batch_acc\n",
    "            step += 1\n",
    "        \n",
    "        train_error[e] /= train_data.num_batches\n",
    "        train_accuracy[e] /= train_data.num_batches\n",
    "\n",
    "        valid_error[e], valid_accuracy[e] = sess.run(\n",
    "            [error, accuracy],\n",
    "            feed_dict={inputs: valid_inputs, targets: valid_targets})\n",
    "\n",
    "    stats = (train_error, train_accuracy, valid_error, valid_accuracy)\n",
    "    ffname = fname+name+\"_10.txt\"\n",
    "    write_to_file(ffname,stats)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropout on all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pickle as pcl\n",
    "from mlp.data_providers import CIFAR10DataProvider, CIFAR100DataProvider\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "#matplotlib inline\n",
    "#from IPython.display import Image\n",
    "\n",
    "\n",
    "assert 'MLP_DATA_DIR' in os.environ, (\n",
    "    'An environment variable MLP_DATA_DIR must be set to the path containing'\n",
    "    ' MLP data before running script.')\n",
    "\n",
    "assert 'OUTPUT_DIR' in os.environ, (\n",
    "    'An environment variable OUTPUT_DIR must be set to the path to write'\n",
    "    ' output to before running script.')\n",
    "\n",
    "# Run a standard CNN to get a model that overfits\n",
    "\n",
    "# Helping functions\n",
    "\n",
    "def set_wb(wb,mp):\n",
    "    weights = {\n",
    "        'wc1': weight_variable([5, 5, 3, 8]),\n",
    "        'wc2': weight_variable([5, 5, 8, 16]),\n",
    "        'wc3': weight_variable([5, 5, 16, 16]),\n",
    "        'wd1': weight_variable([8*8*16, 8**3]),\n",
    "        'out': weight_variable([8**3, 10])\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'bc1': bias_variable([8]),\n",
    "        'bc2': bias_variable([16]),\n",
    "        'bc3': bias_variable([16]),\n",
    "        'bd1': bias_variable([8**3]),\n",
    "        'out': bias_variable([10])\n",
    "    }\n",
    "    return weights, biases\n",
    "\n",
    "def write_to_file(newfile, run_info):\n",
    "    with open(newfile,'w') as outfile:\n",
    "        pcl.dump(run_info,outfile)\n",
    "\n",
    "def read_from_file(newfile):\n",
    "    run_info = pcl.load(open(newfile,'r'))\n",
    "    return run_info\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "# model: CNN/RELU>CNN/RELU>MP>CNN/RELU>MP>FC\n",
    "# DROP ONCE\n",
    "def conv_net(x, weights, biases, drop):\n",
    "    x = tf.reshape(x, shape=[-1, 32, 32, 3])\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = tf.nn.dropout(conv1,drop)\n",
    "    #conv12 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    #conv13 = conv2d(conv12, weights['wc3'], biases['bc3'])\n",
    "    #conv14 = conv2d(conv13, weights['wc3'], biases['bc3'])\n",
    "    # Max Pooling\n",
    "    #conv1 = maxpool2d(conv1, k=2)\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling\n",
    "    conv2 = tf.nn.dropout(conv2,drop)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    # Convolution Layer\n",
    "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n",
    "    # Max Pooling\n",
    "    conv3 = tf.nn.dropout(conv3,drop)\n",
    "    conv3 = maxpool2d(conv3, k=2)\n",
    "    # Fully connected layer\n",
    "    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, drop)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "\n",
    "## session\n",
    "\n",
    "num_epoch = 30 \n",
    "fname = \"baseK5-drop\"\n",
    "\n",
    "train_data = CIFAR10DataProvider('train', batch_size=50)\n",
    "valid_data = CIFAR10DataProvider('valid', batch_size=50)\n",
    "valid_inputs = valid_data.inputs\n",
    "valid_targets = valid_data.to_one_of_k(valid_data.targets)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "pb = [0.1,0.25,0.5,0.75,0.9]\n",
    "for p in pb:\n",
    "    tf.reset_default_graph()\n",
    "    weightsk, biasesk = set_wb(8,2)\n",
    "    \n",
    "    # define model graph\n",
    "    with tf.name_scope('data'):\n",
    "        inputs = tf.placeholder(tf.float32, [None, 3072], name='inputs')\n",
    "        targets = tf.placeholder(tf.float32, [None, 10], name='targets')\n",
    "    #with tf.name_scope('parameters'):\n",
    "    #    weights = tf.Variable(tf.zeros([3072, 10]), name='weights')\n",
    "    #    biases = tf.Variable(tf.zeros([10]), name='biases')\n",
    "    with tf.name_scope('model'):\n",
    "        outputs = conv_net(inputs, weightsk, biasesk,p)\n",
    "    with tf.name_scope('error'):\n",
    "        error = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(outputs,targets))\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer().minimize(error)\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(\n",
    "            tf.argmax(outputs, 1), tf.argmax(targets, 1)), tf.float32))\n",
    "\n",
    "    train_accuracy = np.zeros(num_epoch)\n",
    "    train_error = np.zeros(num_epoch)\n",
    "    valid_accuracy = np.zeros(num_epoch)\n",
    "    valid_error = np.zeros(num_epoch)\n",
    "\n",
    "    name = str(p)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    step = 0\n",
    "    for e in range(num_epoch):\n",
    "        for b, (input_batch, target_batch) in enumerate(train_data):\n",
    "\n",
    "            _, batch_error, batch_acc = sess.run(\n",
    "                [train_step, error, accuracy],\n",
    "                feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "            train_error[e] += batch_error\n",
    "            train_accuracy[e] += batch_acc\n",
    "            step += 1\n",
    "        \n",
    "        train_error[e] /= train_data.num_batches\n",
    "        train_accuracy[e] /= train_data.num_batches\n",
    "\n",
    "        valid_error[e], valid_accuracy[e] = sess.run(\n",
    "            [error, accuracy],\n",
    "            feed_dict={inputs: valid_inputs, targets: valid_targets})\n",
    "\n",
    "    stats = (train_error, train_accuracy, valid_error, valid_accuracy)\n",
    "    ffname = fname+name+\"_10.txt\"\n",
    "    write_to_file(ffname,stats)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pickle as pcl\n",
    "from mlp.data_providers import CIFAR10DataProvider, CIFAR100DataProvider\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "#matplotlib inline\n",
    "#from IPython.display import Image\n",
    "\n",
    "\n",
    "assert 'MLP_DATA_DIR' in os.environ, (\n",
    "    'An environment variable MLP_DATA_DIR must be set to the path containing'\n",
    "    ' MLP data before running script.')\n",
    "\n",
    "assert 'OUTPUT_DIR' in os.environ, (\n",
    "    'An environment variable OUTPUT_DIR must be set to the path to write'\n",
    "    ' output to before running script.')\n",
    "\n",
    "# Run a standard CNN to get a model that overfits\n",
    "\n",
    "# Helping functions\n",
    "\n",
    "def set_wb(wb,mp):\n",
    "    weights = {\n",
    "        'wc1': weight_variable([2, 2, 3, 8]),\n",
    "        'wc2': weight_variable([2, 2, 8, 16]),\n",
    "        'wc3': weight_variable([2, 2, 16, 16]),\n",
    "        'wd1': weight_variable([8*8*16, 8**3]),\n",
    "        'out': weight_variable([8**3, 10])\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'bc1': bias_variable([8]),\n",
    "        'bc2': bias_variable([16]),\n",
    "        'bc3': bias_variable([16]),\n",
    "        'bd1': bias_variable([8**3]),\n",
    "        'out': bias_variable([10])\n",
    "    }\n",
    "    return weights, biases\n",
    "\n",
    "def write_to_file(newfile, run_info):\n",
    "    with open(newfile,'w') as outfile:\n",
    "        pcl.dump(run_info,outfile)\n",
    "\n",
    "def read_from_file(newfile):\n",
    "    run_info = pcl.load(open(newfile,'r'))\n",
    "    return run_info\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "# model: CNN/RELU>CNN/RELU>MP>CNN/RELU>MP>FC\n",
    "# DROP ONCE\n",
    "def conv_net(x, weights, biases, penalty):\n",
    "    x = tf.reshape(x, shape=[-1, 32, 32, 3])\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    l1 = penalty*tf.nn.l2_loss(weights['wc1'])\n",
    "    conv12 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    l12 = penalty*tf.nn.l2_loss(weights['wc2'])\n",
    "    conv13 = conv2d(conv12, weights['wc3'], biases['bc3'])\n",
    "    l13 = penalty*tf.nn.l2_loss(weights['wc3'])\n",
    "    conv14 = conv2d(conv13, weights['wc3'], biases['bc3'])\n",
    "    l14 = penalty*tf.nn.l2_loss(weights['wc3'])\n",
    "    # Max Pooling\n",
    "    #conv1 = maxpool2d(conv1, k=2)\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv14, weights['wc3'], biases['bc3'])\n",
    "\t#conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    l2 = penalty*tf.nn.l2_loss(weights['wc3'])\n",
    "    # Max Pooling\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    # Convolution Layer\n",
    "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n",
    "    l3 = penalty*tf.nn.l2_loss(weights['wc3'])\n",
    "    # Max Pooling\n",
    "    conv3 = maxpool2d(conv3, k=2)\n",
    "    # Fully connected layer\n",
    "    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    l4 = penalty*tf.nn.l2_loss(weights['wd1'])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    #fc1 = tf.nn.dropout(fc1, drop)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    loss = l1 + l12 + l13 + l14 + l2 + l3 + l4\n",
    "    return out,loss\n",
    "\n",
    "\n",
    "## session\n",
    "\n",
    "num_epoch = 30 \n",
    "fname = \"base6CNN-L2\"\n",
    "\n",
    "train_data = CIFAR10DataProvider('train', batch_size=50)\n",
    "valid_data = CIFAR10DataProvider('valid', batch_size=50)\n",
    "valid_inputs = valid_data.inputs\n",
    "valid_targets = valid_data.to_one_of_k(valid_data.targets)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "penalty = [0.1,0.01,0.001,0.0001]\n",
    "for p in penalty:\n",
    "    tf.reset_default_graph()\n",
    "    weightsk, biasesk = set_wb(8,2)\n",
    "    \n",
    "    # define model graph\n",
    "    with tf.name_scope('data'):\n",
    "        inputs = tf.placeholder(tf.float32, [None, 3072], name='inputs')\n",
    "        targets = tf.placeholder(tf.float32, [None, 10], name='targets')\n",
    "    #with tf.name_scope('parameters'):\n",
    "    #    weights = tf.Variable(tf.zeros([3072, 10]), name='weights')\n",
    "    #    biases = tf.Variable(tf.zeros([10]), name='biases')\n",
    "    with tf.name_scope('model'):\n",
    "        outputs,loss = conv_net(inputs, weightsk, biasesk,p)\n",
    "    with tf.name_scope('error'):\n",
    "        error = tf.reduce_mean(tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(outputs, targets)) + loss)\n",
    "    with tf.name_scope('error_val'):\n",
    "        error_val = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(outputs, targets))\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer().minimize(error)\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(\n",
    "            tf.argmax(outputs, 1), tf.argmax(targets, 1)), tf.float32))\n",
    "\n",
    "    train_accuracy = np.zeros(num_epoch)\n",
    "    train_error = np.zeros(num_epoch)\n",
    "    valid_accuracy = np.zeros(num_epoch)\n",
    "    valid_error = np.zeros(num_epoch)\n",
    "\n",
    "    name = str(p)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    step = 0\n",
    "    for e in range(num_epoch):\n",
    "        for b, (input_batch, target_batch) in enumerate(train_data):\n",
    "\n",
    "            _, batch_error, batch_acc = sess.run(\n",
    "                [train_step, error, accuracy],\n",
    "                feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "            train_error[e] += batch_error\n",
    "            train_accuracy[e] += batch_acc\n",
    "            step += 1\n",
    "        \n",
    "        train_error[e] /= train_data.num_batches\n",
    "        train_accuracy[e] /= train_data.num_batches\n",
    "\n",
    "        valid_error[e], valid_accuracy[e] = sess.run(\n",
    "            [error_val, accuracy],\n",
    "            feed_dict={inputs: valid_inputs, targets: valid_targets})\n",
    "\n",
    "    stats = (train_error, train_accuracy, valid_error, valid_accuracy)\n",
    "    ffname = fname+name+\"_10.txt\"\n",
    "    write_to_file(ffname,stats)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pickle as pcl\n",
    "from mlp.data_providers import CIFAR10DataProvider, CIFAR100DataProvider\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "#matplotlib inline\n",
    "#from IPython.display import Image\n",
    "\n",
    "\n",
    "assert 'MLP_DATA_DIR' in os.environ, (\n",
    "    'An environment variable MLP_DATA_DIR must be set to the path containing'\n",
    "    ' MLP data before running script.')\n",
    "assert 'OUTPUT_DIR' in os.environ, (\n",
    "    'An environment variable OUTPUT_DIR must be set to the path to write'\n",
    "    ' output to before running script.')\n",
    "\n",
    "def set_wb(wb,mp):\n",
    "    p = 32/(mp*2)\n",
    "    weights = {\n",
    "        'wc1': weight_variable([5, 5, 3, wb]),\n",
    "        'wc2': weight_variable([5, 5, wb, wb*2]),\n",
    "        'wc3': weight_variable([5, 5, wb*2, wb*2]),\n",
    "        'wd1': weight_variable([p*p*(wb*2), wb**3]),\n",
    "        'out': weight_variable([wb**3, 10])\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'bc1': bias_variable([wb]),\n",
    "        'bc2': bias_variable([wb*2]),\n",
    "        'bc3': bias_variable([wb*2]),\n",
    "        'bd1': bias_variable([wb**3]),\n",
    "        'out': bias_variable([10])\n",
    "    }\n",
    "    return weights, biases\n",
    "\n",
    "def write_to_file(newfile, run_info):\n",
    "    with open(newfile,'w') as outfile:\n",
    "        pcl.dump(run_info,outfile)\n",
    "\n",
    "def read_from_file(newfile):\n",
    "    run_info = pcl.load(open(newfile,'r'))\n",
    "    return run_info\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "# model: CNN/RELU>CNN/RELU>MP>CNN/RELU>MP>FC\n",
    "\n",
    "def conv_net(x, weights, biases):\n",
    "    x = tf.reshape(x, shape=[-1, 32, 32, 3])\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling\n",
    "    #conv1 = maxpool2d(conv1, k=2)\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    # Convolution Layer\n",
    "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n",
    "    # Max Pooling\n",
    "    conv3 = maxpool2d(conv3, k=2)\n",
    "    # Fully connected layer\n",
    "    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "\n",
    "## TRANFORMATIONS\n",
    "\n",
    "def pre_process_image(image):\n",
    "    image = tf.reshape(image,[32,32,3])\n",
    "\n",
    "    choice = random.randint(0, 3)\n",
    "    if (choice == 0):\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        #image = tf.image.random_flip_up_down(image)\n",
    "        # Randomly adjust hue, contrast and saturation.\n",
    "        image = tf.image.random_hue(image, max_delta=0.05)\n",
    "        image = tf.image.random_contrast(image, lower=0.3, upper=1.0)\n",
    "        image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "        image = tf.image.random_saturation(image, lower=0.0, upper=2.0)\n",
    "\n",
    "        # Limit the image pixels between [0, 1] in case of overflow.\n",
    "        image = tf.minimum(image, 1.0)\n",
    "        image = tf.maximum(image, 0.0)\n",
    "\n",
    "    return tf.reshape(image,[-1])\n",
    "\n",
    "def pre_process_image_flip_ud(image):\n",
    "    image = tf.reshape(image,[32,32,3])\n",
    "    choice = random.randint(0, 3)\n",
    "    if (choice == 0):\n",
    "        image = tf.image.random_flip_up_down(image)\n",
    "        # Limit the image pixels between [0, 1] in case of overflow.\n",
    "        image = tf.minimum(image, 1.0)\n",
    "        image = tf.maximum(image, 0.0)\n",
    "\n",
    "    return tf.reshape(image,[-1])\n",
    "\n",
    "def pre_process_image_flip_lr(image):\n",
    "    image = tf.reshape(image,[32,32,3])\n",
    "    choice = random.randint(0, 3)\n",
    "    if (choice == 0):\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        # Limit the image pixels between [0, 1] in case of overflow.\n",
    "        image = tf.minimum(image, 1.0)\n",
    "        image = tf.maximum(image, 0.0)\n",
    "\n",
    "    return tf.reshape(image,[-1])\n",
    "\n",
    "def pre_process_image_hue(image):\n",
    "    image = tf.reshape(image,[32,32,3])\n",
    "    # Randomly adjust hue, contrast and saturation.\n",
    "    choice = random.randint(0, 3)\n",
    "    if (choice == 0):\n",
    "        image = tf.image.random_hue(image, max_delta=0.05)\n",
    "        # Limit the image pixels between [0, 1] in case of overflow.\n",
    "        image = tf.minimum(image, 1.0)\n",
    "        image = tf.maximum(image, 0.0)\n",
    "    return tf.reshape(image,[-1])\n",
    "\n",
    "def pre_process_image_contrast(image):\n",
    "    image = tf.reshape(image,[32,32,3])\n",
    "\n",
    "    choice = random.randint(0, 3)\n",
    "    if (choice == 0):\n",
    "        image = tf.image.random_contrast(image, lower=0.3, upper=1.0)\n",
    "        # Limit the image pixels between [0, 1] in case of overflow.\n",
    "        image = tf.minimum(image, 1.0)\n",
    "        image = tf.maximum(image, 0.0)\n",
    "\n",
    "    return tf.reshape(image,[-1])\n",
    "\n",
    "def pre_process_image_brightness(image):\n",
    "    image = tf.reshape(image,[32,32,3])\n",
    "\n",
    "    choice = random.randint(0, 3)\n",
    "    if (choice == 0):\n",
    "        image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "        # Limit the image pixels between [0, 1] in case of overflow.\n",
    "        image = tf.minimum(image, 1.0)\n",
    "        image = tf.maximum(image, 0.0)\n",
    "\n",
    "    return tf.reshape(image,[-1])\n",
    "\n",
    "def pre_process_image_saturation(image):\n",
    "    image = tf.reshape(image,[32,32,3])\n",
    "    choice = random.randint(0, 3)\n",
    "    if (choice == 0):\n",
    "        # Randomly adjust hue, contrast and saturation.\n",
    "        image = tf.image.random_saturation(image, lower=0.0, upper=2.0)\n",
    "        # Limit the image pixels between [0, 1] in case of overflow.\n",
    "        image = tf.minimum(image, 1.0)\n",
    "        image = tf.maximum(image, 0.0)\n",
    "\n",
    "    return tf.reshape(image,[-1])\n",
    "\n",
    "def pre_process(x, trans):\n",
    "    if (trans == 'flip_ud'):\n",
    "        x = tf.map_fn(lambda image: pre_process_image_flip_ud(image), x)\n",
    "    elif (trans == 'flip_lr'):\n",
    "        x = tf.map_fn(lambda image: pre_process_image_flip_lr(image), x)\n",
    "    elif (trans == 'hue'):\n",
    "        x = tf.map_fn(lambda image: pre_process_image_hue(image), x)\n",
    "    elif (trans == 'contrast'):\n",
    "        x = tf.map_fn(lambda image: pre_process_image_contrast(image), x)\n",
    "    elif (trans == 'brightness'):\n",
    "        x = tf.map_fn(lambda image: pre_process_image_brightness(image), x)\n",
    "    elif (trans == 'saturation'):\n",
    "        x = tf.map_fn(lambda image: pre_process_image_saturation(image), x)\n",
    "    else:\n",
    "        print('No transformation used')\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "## session\n",
    "\n",
    "num_epoch = 30\n",
    "fname = \"baseDAK5-\"\n",
    "\n",
    "train_data = CIFAR10DataProvider('train', batch_size=50)\n",
    "valid_data = CIFAR10DataProvider('valid', batch_size=50)\n",
    "valid_inputs = valid_data.inputs\n",
    "valid_targets = valid_data.to_one_of_k(valid_data.targets)\n",
    "\n",
    "weightsk, biasesk = set_wb(8,2)\n",
    "\n",
    "transf = ['flip_ud', 'flip_lr', 'hue','contrast','brightness','saturation']\n",
    "for trans in transf:\n",
    "    tf.reset_default_graph()\n",
    "    weightsk, biasesk = set_wb(8,2)\n",
    "\n",
    "    with tf.name_scope('data'):\n",
    "        inputs = tf.placeholder(tf.float32, [None, 3072], name='inputs')\n",
    "        targets = tf.placeholder(tf.float32, [None, 10], name='targets')\n",
    "        preprocessed_inputs = tf.placeholder(tf.float32, [None, train_data.inputs.shape[1]], name='dinputs')\n",
    "        distorted_inputs = pre_process(preprocessed_inputs, trans)\n",
    "    with tf.name_scope('model'):\n",
    "        outputs = conv_net(inputs, weightsk, biasesk)\n",
    "    with tf.name_scope('error'):\n",
    "        error = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(outputs,targets))\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer().minimize(error)\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(\n",
    "            tf.argmax(outputs, 1), tf.argmax(targets, 1)), tf.float32))\n",
    "\n",
    "    train_accuracy = np.zeros(num_epoch)\n",
    "    train_error = np.zeros(num_epoch)\n",
    "    valid_accuracy = np.zeros(num_epoch)\n",
    "    valid_error = np.zeros(num_epoch)\n",
    "\n",
    "    name = str(trans)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    step = 0\n",
    "    for e in range(num_epoch):\n",
    "        for b, (input_batch, target_batch) in enumerate(train_data):\n",
    "            # do train step with current batch\n",
    "            input_batch_dist = sess.run(distorted_inputs, feed_dict={preprocessed_inputs: input_batch})\n",
    "            _ = sess.run(\n",
    "                [train_step],\n",
    "                feed_dict={inputs: input_batch_dist, targets: target_batch})\n",
    "            batch_error, batch_acc = sess.run(\n",
    "                [error, accuracy],\n",
    "                feed_dict={inputs: input_batch, targets: target_batch})\n",
    "            train_error[e] += batch_error\n",
    "            train_accuracy[e] += batch_acc\n",
    "            step += 1\n",
    "        train_error[e] /= train_data.num_batches\n",
    "        train_accuracy[e] /= train_data.num_batches\n",
    "        valid_error[e], valid_accuracy[e] = sess.run(\n",
    "            [error, accuracy],\n",
    "            feed_dict={inputs: valid_inputs, targets: valid_targets})\n",
    "\n",
    "    stats = (train_error, train_accuracy, valid_error, valid_accuracy)\n",
    "    ffname = fname+name+\"_10.txt\"\n",
    "    write_to_file(ffname,stats)\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The results were plotted with input the txt files"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mlp]",
   "language": "python",
   "name": "conda-env-mlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
